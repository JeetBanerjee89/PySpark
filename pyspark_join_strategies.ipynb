{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984ca4e3-8adc-44d7-888c-108a42714d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark initialization\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36321063-074e-4271-aac7-28aa35df2878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LIN-5CG1349BDL.corp.capgemini.com:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x213d1864af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building spark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9d70a-e6c7-45cc-af4e-1671ba215996",
   "metadata": {},
   "source": [
    "# Pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf6cb7b-c77d-468c-87d2-5f0dccaf0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark chooses a join strategy based on the size of the data. \n",
    "# To avoid costly shuffle and sort operations, it favors hash-based join strategies, especially when data can be broadcasted.\n",
    "# Spark supports both Equi Join (using “=”) and Non-Equi Join (using “<,” “>”, “≥,” “≤”)\n",
    "# clusters: connecting multiple computers (desktops/laptops) through network. 20 core per machine, and 100GB RAM in each. Hence, for 10 clusters, 200 core and 1000 GB (1 TB) RAM.\n",
    "# Clusters are working in master-slave architecture; slaves are the worker nodes.\n",
    "# Master node consists of YARN (Yet Another Resource Negotiator), and resource manager, whereas, worker nodes consist Node Manager\n",
    "# Spark Architecture: https://youtu.be/xJVonk4yxJY?si=jiXGgEfHymD_sy0Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39c4838-cb0c-4335-b78c-1984988382a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|client_id|   name|\n",
      "+---------+-------+\n",
      "|        0|client1|\n",
      "|        1|client2|\n",
      "|        2|client3|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe_1\n",
    "df_clients = [\n",
    "    (0, \"client1\"),\n",
    "    (1, \"client2\"),\n",
    "    (2, \"client3\")]\n",
    "columns = (\"client_id\", \"name\")\n",
    "df_clients = spark.createDataFrame(df_clients, columns)\n",
    "df_clients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12dc42e-81f8-46a2-9f57-b74cbaa3cf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+\n",
      "|client_id|order_id|order_amount|\n",
      "+---------+--------+------------+\n",
      "|        0|  order1|         100|\n",
      "|        1|  order2|         200|\n",
      "|        2|  order3|         150|\n",
      "+---------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe_2\n",
    "df_orders = [\n",
    "    (0, \"order1\", 100),\n",
    "    (1, \"order2\", 200),\n",
    "    (2, \"order3\", 150)]\n",
    "columns = (\"client_id\", \"order_id\", \"order_amount\")\n",
    "df_orders = spark.createDataFrame(df_orders, columns)\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3e9eb-0f37-4372-ad59-f5885df94f1d",
   "metadata": {},
   "source": [
    "# Broadcast Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c175c435-83d9-4e22-b8f5-894fde13fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It’s ideal when one DataFrame is small enough to fit in the memory of each executor.\n",
    "# Spark broadcasts the smaller DataFrame to all workers.\n",
    "# This minimizes data shuffling and accelerates the join operation, as the join occurs within the same node, resulting in a decrease in network overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd95c83-4095-4157-a3f5-639ab55ab3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+------------+\n",
      "|client_id|   name|order_id|order_amount|\n",
      "+---------+-------+--------+------------+\n",
      "|        0|client1|  order1|         100|\n",
      "|        1|client2|  order2|         200|\n",
      "|        2|client3|  order3|         150|\n",
      "+---------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcast_df = df_clients.hint('BROADCAST').join(df_orders, on = \"client_id\", how = \"inner\")\n",
    "# broadcast_df.explain(mode=\"formatted\")\n",
    "broadcast_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4ce63-af35-4197-88f3-b25fcdd872bd",
   "metadata": {},
   "source": [
    "# Shuffle Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5bec3d-0d6e-4964-a9f1-cd1eb03817cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It’s suitable when neither of the joined tables can fit in memory.\n",
    "# It involves a shuffle phase, where data is redistributed across partitions based on the join key.\n",
    "# Be careful when using this strategy, as it may incur higher network and disk I/O costs, which largely decreases the performance due to the full shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820b0c12-3e18-4a7c-933a-7dcbeaa9c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+------------+\n",
      "|client_id|   name|order_id|order_amount|\n",
      "+---------+-------+--------+------------+\n",
      "|        0|client1|  order1|         100|\n",
      "|        1|client2|  order2|         200|\n",
      "|        2|client3|  order3|         150|\n",
      "+---------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shuffle_hash_df = df_clients.hint(\"SHUFFLE_HASH\").join(df_orders, on = \"client_id\", how = \"inner\")\n",
    "shuffle_hash_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5e02a-4b99-48ee-99ea-19c740a5e2de",
   "metadata": {},
   "source": [
    "# Sort Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f609b45-9a87-4671-bae8-7918f4b3773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It’s appropriate when both tables are large and cannot fit in the memory.\n",
    "# It Involves sorting both tables based on the join key and then merging them.\n",
    "# It provides good performance for certain types of queries but requires sorting, which can be computationally expensive.\n",
    "# Shuffle: The data from both tables is partitioned based on the join key. This partitioning ensures that records with the same join key are directed to the same partition.\n",
    "# Sort: Within each partition, the data is then sorted based on the join key.\n",
    "# Merge: The sorted data is subsequently merged across partitions to execute the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca6bf20-70dc-438e-9f67-bf49db37bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+------------+\n",
      "|client_id|   name|order_id|order_amount|\n",
      "+---------+-------+--------+------------+\n",
      "|        0|client1|  order1|         100|\n",
      "|        1|client2|  order2|         200|\n",
      "|        2|client3|  order3|         150|\n",
      "+---------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sort_merge_join_df = df_clients.hint(\"MERGE\").join(df_orders, on = \"client_id\", how = \"inner\")\n",
    "sort_merge_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7a4fc-466c-493e-beef-26327ab00866",
   "metadata": {},
   "source": [
    "# Cartesian Product Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4837a4ec-19c8-4d62-bf49-8cc6e1c420be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It involves joining every row from the first table with every row from the second table, making it highly resource-intensive. \n",
    "# This strategy should be avoided for large datasets due to a significant increase in the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4c6632a-aa1a-4f65-9535-1b96b4400c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+------------+\n",
      "|client_id|   name|order_id|order_amount|\n",
      "+---------+-------+--------+------------+\n",
      "|        0|client1|  order1|         100|\n",
      "|        1|client2|  order2|         200|\n",
      "|        2|client3|  order3|         150|\n",
      "+---------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cartesian_product_join_df = df_clients.hint(\"SHUFFLE_REPLICATE_NL\").join(df_orders, on = \"client_id\", how = \"inner\")\n",
    "cartesian_product_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee864569-ffd8-41d4-b516-f902579a04d3",
   "metadata": {},
   "source": [
    "# Broadcast Nested Loop Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3d39575-16e2-484f-bbfa-ba3d0ff102bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It’s useful when joining a large table with a small table that doesn’t fit in memory but has a filter condition. \n",
    "# The smaller table is broadcasted, and a nested loop is used to join matching records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be93079-5d63-455b-9607-f90252a51cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+--------+------------+\n",
      "|client_id|   name|client_id|order_id|order_amount|\n",
      "+---------+-------+---------+--------+------------+\n",
      "|        0|client1|        0|  order1|         100|\n",
      "|        1|client2|        0|  order1|         100|\n",
      "|        2|client3|        0|  order1|         100|\n",
      "|        0|client1|        1|  order2|         200|\n",
      "|        1|client2|        1|  order2|         200|\n",
      "|        2|client3|        1|  order2|         200|\n",
      "|        0|client1|        2|  order3|         150|\n",
      "|        1|client2|        2|  order3|         150|\n",
      "|        2|client3|        2|  order3|         150|\n",
      "+---------+-------+---------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "broadcast_nested_loop_join_df = df_clients.hint(\"BROADCAST\").join(df_orders)\n",
    "broadcast_nested_loop_join_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

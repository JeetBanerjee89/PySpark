{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377dc746-9e93-4a05-8780-304cbd64aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark initialization\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130951db-2af6-483b-a1b4-8765e937241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sqlfunc\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b327a65-779e-4505-b682-f39f74d2e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building sparksession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc63b2fc-b4d4-48af-b6c3-139b0ed3168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+-------------+---+----------+------+\n",
      "|Employee_Name|Age|Department|Salary|\n",
      "+-------------+---+----------+------+\n",
      "|          Ram| 28|     Sales|  3000|\n",
      "|        Meena| 33|     Sales|  4600|\n",
      "|        Robin| 40|     Sales|  4100|\n",
      "|        Kunal| 25|   Finance|  3000|\n",
      "|          Ram| 28|     Sales|  3000|\n",
      "|      Srishti| 46|Management|  3300|\n",
      "|         Jeny| 26|   Finance|  3900|\n",
      "|       Hitesh| 30| Marketing|  3000|\n",
      "|      Kailash| 29| Marketing|  2000|\n",
      "|       Sharad| 39|     Sales|  4100|\n",
      "+-------------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sample_data\n",
    "sampleData = ((\"Ram\", 28, \"Sales\", 3000),\n",
    "              (\"Meena\", 33, \"Sales\", 4600),\n",
    "              (\"Robin\", 40, \"Sales\", 4100),\n",
    "              (\"Kunal\", 25, \"Finance\", 3000),\n",
    "              (\"Ram\", 28, \"Sales\", 3000),\n",
    "              (\"Srishti\", 46, \"Management\", 3300),\n",
    "              (\"Jeny\", 26, \"Finance\", 3900),\n",
    "              (\"Hitesh\", 30, \"Marketing\", 3000),\n",
    "              (\"Kailash\", 29, \"Marketing\", 2000),\n",
    "              (\"Sharad\", 39, \"Sales\", 4100)\n",
    "              )\n",
    "columns = [\"Employee_Name\", \"Age\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data=sampleData, schema=columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28046807-4498-45ce-a80a-587f483517a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining window partition\n",
    "from pyspark.sql.window import Window\n",
    "windowPartition = Window.partitionBy(\"Department\").orderBy(\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638be424-c74e-456c-ae47-bcc357b4bd13",
   "metadata": {},
   "source": [
    "# Analytical Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c7c5b5-8446-432f-be54-2b1d139544a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+---------+\n",
      "|Employee_Name|Age|Department|Salary|cume_dist|\n",
      "+-------------+---+----------+------+---------+\n",
      "|        Kunal| 25|   Finance|  3000|      0.5|\n",
      "|         Jeny| 26|   Finance|  3900|      1.0|\n",
      "|      Srishti| 46|Management|  3300|      1.0|\n",
      "|      Kailash| 29| Marketing|  2000|      0.5|\n",
      "|       Hitesh| 30| Marketing|  3000|      1.0|\n",
      "|          Ram| 28|     Sales|  3000|      0.4|\n",
      "|          Ram| 28|     Sales|  3000|      0.4|\n",
      "|        Meena| 33|     Sales|  4600|      0.6|\n",
      "|       Sharad| 39|     Sales|  4100|      0.8|\n",
      "|        Robin| 40|     Sales|  4100|      1.0|\n",
      "+-------------+---+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#analytical window function\n",
    "# cume_dist() window function is used to get the cumulative distribution within a window partition\n",
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\", cume_dist().over(windowPartition)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e101b2-3663-4193-a165-501736d4287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+----+\n",
      "|Employee_Name|Age|Department|Salary| lag|\n",
      "+-------------+---+----------+------+----+\n",
      "|        Kunal| 25|   Finance|  3000|NULL|\n",
      "|         Jeny| 26|   Finance|  3900|3000|\n",
      "|      Srishti| 46|Management|  3300|NULL|\n",
      "|      Kailash| 29| Marketing|  2000|NULL|\n",
      "|       Hitesh| 30| Marketing|  3000|2000|\n",
      "|          Ram| 28|     Sales|  3000|NULL|\n",
      "|          Ram| 28|     Sales|  3000|3000|\n",
      "|        Meena| 33|     Sales|  4600|3000|\n",
      "|       Sharad| 39|     Sales|  4100|4600|\n",
      "|        Robin| 40|     Sales|  4100|4100|\n",
      "+-------------+---+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#analytical window function\n",
    "# lag() function: access previous rowsâ€™ data as per the defined offset value in the function\n",
    "from pyspark.sql.functions import lag\n",
    "df.withColumn('lag', lag('Salary', 1).over(windowPartition)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9ff218-2201-4fd0-8114-2b03d9de98cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+----+\n",
      "|Employee_Name|Age|Department|Salary|lead|\n",
      "+-------------+---+----------+------+----+\n",
      "|        Kunal| 25|   Finance|  3000|3900|\n",
      "|         Jeny| 26|   Finance|  3900|NULL|\n",
      "|      Srishti| 46|Management|  3300|NULL|\n",
      "|      Kailash| 29| Marketing|  2000|3000|\n",
      "|       Hitesh| 30| Marketing|  3000|NULL|\n",
      "|          Ram| 28|     Sales|  3000|3000|\n",
      "|          Ram| 28|     Sales|  3000|4600|\n",
      "|        Meena| 33|     Sales|  4600|4100|\n",
      "|       Sharad| 39|     Sales|  4100|4100|\n",
      "|        Robin| 40|     Sales|  4100|NULL|\n",
      "+-------------+---+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#analytical window function\n",
    "# lead() function: access next rows data as per the defined offset value in the function\n",
    "from pyspark.sql.functions import lead\n",
    "df.withColumn(\"lead\", lead(\"Salary\", 1).over(windowPartition)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93485218-9ad9-4b42-9708-ca942e208c3c",
   "metadata": {},
   "source": [
    "# Ranking Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66fa3ea4-e0c4-4a17-a0b1-ec19c4928260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Roll_No: long (nullable = true)\n",
      " |-- Student_Name: string (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- Marks: long (nullable = true)\n",
      "\n",
      "+-------+------------+--------------+-----+\n",
      "|Roll_No|Student_Name|       Subject|Marks|\n",
      "+-------+------------+--------------+-----+\n",
      "|    101|         Ram|       Biology|   80|\n",
      "|    103|       Meena|Social Science|   78|\n",
      "|    104|       Robin|      Sanskrit|   58|\n",
      "|    102|       Kunal|       Physics|   89|\n",
      "|    101|         Ram|       Biology|   80|\n",
      "|    106|      Hitesh|         Maths|   88|\n",
      "|    108|        Jeny|       Physics|   75|\n",
      "|    107|      Hitesh|         Maths|   88|\n",
      "|    109|     Kailash|         Maths|   90|\n",
      "|    105|      Sharad|Social Science|   84|\n",
      "+-------+------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sample_data\n",
    "sampleData = ((101, \"Ram\", \"Biology\", 80),\n",
    "              (103, \"Meena\", \"Social Science\", 78),\n",
    "              (104, \"Robin\", \"Sanskrit\", 58),\n",
    "              (102, \"Kunal\", \"Physics\", 89),\n",
    "              (101, \"Ram\", \"Biology\", 80),\n",
    "              (106, \"Hitesh\", \"Maths\", 88),\n",
    "              (108, \"Jeny\", \"Physics\", 75),\n",
    "              (107, \"Hitesh\", \"Maths\", 88),\n",
    "              (109, \"Kailash\", \"Maths\", 90),\n",
    "              (105, \"Sharad\", \"Social Science\", 84)\n",
    "              )\n",
    "columns = [\"Roll_No\", \"Student_Name\", \"Subject\", \"Marks\"]\n",
    "df2 = spark.createDataFrame(data=sampleData, schema=columns)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095f0ff6-3d00-442a-b7b4-9bad7268c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining window partition\n",
    "from pyspark.sql.window import Window\n",
    "windowPartition1 = Window.partitionBy(\"Subject\").orderBy(\"Marks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7d50b08-1c17-4892-ba81-721e6509acc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|row_number|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    101|         Ram|       Biology|   80|         2|\n",
      "|    106|      Hitesh|         Maths|   88|         1|\n",
      "|    107|      Hitesh|         Maths|   88|         2|\n",
      "|    109|     Kailash|         Maths|   90|         3|\n",
      "|    108|        Jeny|       Physics|   75|         1|\n",
      "|    102|       Kunal|       Physics|   89|         2|\n",
      "|    104|       Robin|      Sanskrit|   58|         1|\n",
      "|    103|       Meena|Social Science|   78|         1|\n",
      "|    105|      Sharad|Social Science|   84|         2|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ranking window function\n",
    "# row_number(): used to give a sequential number to each row present in the table\n",
    "from pyspark.sql.functions import row_number\n",
    "df2.withColumn(\"row_number\", row_number().over(windowPartition1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4c739d-7a7e-4495-9894-bea1b067ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----+\n",
      "|Roll_No|Student_Name|       Subject|Marks|rank|\n",
      "+-------+------------+--------------+-----+----+\n",
      "|    101|         Ram|       Biology|   80|   1|\n",
      "|    101|         Ram|       Biology|   80|   1|\n",
      "|    106|      Hitesh|         Maths|   88|   1|\n",
      "|    107|      Hitesh|         Maths|   88|   1|\n",
      "|    109|     Kailash|         Maths|   90|   3|\n",
      "|    108|        Jeny|       Physics|   75|   1|\n",
      "|    102|       Kunal|       Physics|   89|   2|\n",
      "|    104|       Robin|      Sanskrit|   58|   1|\n",
      "|    103|       Meena|Social Science|   78|   1|\n",
      "|    105|      Sharad|Social Science|   84|   2|\n",
      "+-------+------------+--------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ranking window function\n",
    "# rank(): used to give ranks to rows specified in the window partition\n",
    "# leaves gaps in rank if there are ties\n",
    "from pyspark.sql.functions import rank\n",
    "df2.withColumn(\"rank\", rank().over(windowPartition1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1889606-6105-4baf-8536-0988df426159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+------------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|percent_rank|\n",
      "+-------+------------+--------------+-----+------------+\n",
      "|    101|         Ram|       Biology|   80|         0.0|\n",
      "|    101|         Ram|       Biology|   80|         0.0|\n",
      "|    106|      Hitesh|         Maths|   88|         0.0|\n",
      "|    107|      Hitesh|         Maths|   88|         0.0|\n",
      "|    109|     Kailash|         Maths|   90|         1.0|\n",
      "|    108|        Jeny|       Physics|   75|         0.0|\n",
      "|    102|       Kunal|       Physics|   89|         1.0|\n",
      "|    104|       Robin|      Sanskrit|   58|         0.0|\n",
      "|    103|       Meena|Social Science|   78|         0.0|\n",
      "|    105|      Sharad|Social Science|   84|         1.0|\n",
      "+-------+------------+--------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ranking window function\n",
    "#percent_rank(): provides rank to rows in a percentile format\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df2.withColumn(\"percent_rank\", percent_rank().over(windowPartition1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670a5060-d964-4662-b2a5-81b21690c6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|dense_rank|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    106|      Hitesh|         Maths|   88|         1|\n",
      "|    107|      Hitesh|         Maths|   88|         1|\n",
      "|    109|     Kailash|         Maths|   90|         2|\n",
      "|    108|        Jeny|       Physics|   75|         1|\n",
      "|    102|       Kunal|       Physics|   89|         2|\n",
      "|    104|       Robin|      Sanskrit|   58|         1|\n",
      "|    103|       Meena|Social Science|   78|         1|\n",
      "|    105|      Sharad|Social Science|   84|         2|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ranking window function\n",
    "# dense_rank(): similar to rank() function, there is only one difference - dense_rank() function doesn't leave gaps in rank when there are ties\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df2.withColumn(\"dense_rank\", dense_rank().over(windowPartition1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c0576-0245-4f3b-bc05-96538aa4d31a",
   "metadata": {},
   "source": [
    "# Aggregate Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bbbda58-a9fb-4934-a41e-acd280b26f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|Employee_Name|Department|Salary|\n",
      "+-------------+----------+------+\n",
      "|          Ram|     Sales|  3000|\n",
      "|        Meena|     Sales|  4600|\n",
      "|        Robin|     Sales|  4100|\n",
      "|        Kunal|   Finance|  3000|\n",
      "|          Ram|     Sales|  3000|\n",
      "|      Srishti|Management|  3300|\n",
      "|         Jeny|   Finance|  3900|\n",
      "|       Hitesh| Marketing|  3000|\n",
      "|      Kailash| Marketing|  2000|\n",
      "|       Sharad|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sample_data\n",
    "sampleData = ((\"Ram\", \"Sales\", 3000),\n",
    "              (\"Meena\", \"Sales\", 4600),\n",
    "              (\"Robin\", \"Sales\", 4100),\n",
    "              (\"Kunal\", \"Finance\", 3000),\n",
    "              (\"Ram\", \"Sales\", 3000),\n",
    "              (\"Srishti\", \"Management\", 3300),\n",
    "              (\"Jeny\", \"Finance\", 3900),\n",
    "              (\"Hitesh\", \"Marketing\", 3000),\n",
    "              (\"Kailash\", \"Marketing\", 2000),\n",
    "              (\"Sharad\", \"Sales\", 4100)\n",
    "              )\n",
    "columns = [\"Employee_Name\", \"Department\", \"Salary\"]\n",
    "df3 = spark.createDataFrame(data=sampleData, schema=columns)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d0122cf-ed12-41d0-8f14-f9c07a0a7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining window partition\n",
    "from pyspark.sql.window import Window\n",
    "windowPartitionAgg = Window.partitionBy(\"Department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44eebace-1b2f-4a13-9b31-82f8d1b91339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+-----+----+----+\n",
      "|Employee_Name|Department|Salary|   Avg|  Sum| Min| Max|\n",
      "+-------------+----------+------+------+-----+----+----+\n",
      "|        Kunal|   Finance|  3000|3450.0| 6900|3000|3900|\n",
      "|         Jeny|   Finance|  3900|3450.0| 6900|3000|3900|\n",
      "|      Srishti|Management|  3300|3300.0| 3300|3300|3300|\n",
      "|       Hitesh| Marketing|  3000|2500.0| 5000|2000|3000|\n",
      "|      Kailash| Marketing|  2000|2500.0| 5000|2000|3000|\n",
      "|          Ram|     Sales|  3000|3760.0|18800|3000|4600|\n",
      "|        Meena|     Sales|  4600|3760.0|18800|3000|4600|\n",
      "|        Robin|     Sales|  4100|3760.0|18800|3000|4600|\n",
      "|          Ram|     Sales|  3000|3760.0|18800|3000|4600|\n",
      "|       Sharad|     Sales|  4100|3760.0|18800|3000|4600|\n",
      "+-------------+----------+------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#aggregate window fundtions - avg, sum, min, max\n",
    "from pyspark.sql.functions import col,avg,sum,min,max\n",
    "df3.withColumn(\"Avg\", avg(col(\"Salary\")).over(windowPartitionAgg))\\\n",
    ".withColumn(\"Sum\", sum(col(\"Salary\")).over(windowPartitionAgg))\\\n",
    ".withColumn(\"Min\", min(col(\"Salary\")).over(windowPartitionAgg))\\\n",
    ".withColumn(\"Max\", max(col(\"Salary\")).over(windowPartitionAgg)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0ebf1a-6688-4a08-99e2-47cef7a3b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark initialization\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba78beb-bc8d-482d-902d-79f90284ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building spark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89eefc9c-84c0-43c7-a472-cd7938881b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|Employee_Name|CGI_Location|\n",
      "+-------------+------------+\n",
      "|            A|   Bengaluru|\n",
      "|            B| Bhubaneswar|\n",
      "|            C|     Chennai|\n",
      "|            D|  Coimbatore|\n",
      "|            E|Ghandhinagar|\n",
      "|            F|   Bengaluru|\n",
      "|            G| Bhubaneswar|\n",
      "|            H|     Chennai|\n",
      "|            I|  Coimbatore|\n",
      "|            J|Ghandhinagar|\n",
      "|            K|   Bengaluru|\n",
      "|            L| Bhubaneswar|\n",
      "|            M|     Chennai|\n",
      "|            N|  Coimbatore|\n",
      "|            O|Ghandhinagar|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"test_data.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a09555b-000b-4dae-8a5a-67e50620ee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|CGI_Location|Employees|\n",
      "+------------+---------+\n",
      "| Bhubaneswar|  G, B, L|\n",
      "|     Chennai|  M, C, H|\n",
      "|Ghandhinagar|  J, E, O|\n",
      "|  Coimbatore|  I, N, D|\n",
      "|   Bengaluru|  F, K, A|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby 'CGI_Location' to get concatenated list of employees\n",
    "import pyspark.sql.functions as sqlfunc\n",
    "employee_location = df.groupBy('CGI_Location').agg(sqlfunc.concat_ws(', ', sqlfunc.collect_set(df.Employee_Name)).alias('Employees'))\n",
    "employee_location.show()                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e30a37-9efb-44cc-b491-03c71a50edfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|CGI_Location|Employees|\n",
      "+------------+---------+\n",
      "|   Bengaluru|  F, K, A|\n",
      "| Bhubaneswar|  G, B, L|\n",
      "|     Chennai|  M, C, H|\n",
      "|  Coimbatore|  I, N, D|\n",
      "|Ghandhinagar|  J, E, O|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sorting 'CGI_Location'\n",
    "from pyspark.sql.functions import *\n",
    "employee_location = employee_location.sort(col('CGI_Location').asc())\n",
    "employee_location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b52f3f-a170-4a3b-8940-f7e37067a3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|    name|  numbers|\n",
      "+--------+---------+\n",
      "|    Amit|[1, 2, 3]|\n",
      "|Prashant|[4, 5, 6]|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame creation to demonstate 'getItem' method\n",
    "data = [\n",
    "    (\"Amit\", [1, 2, 3]),\n",
    "    (\"Prashant\", [4, 5, 6])]\n",
    "columns = [\"name\", \"numbers\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d9cbd5-4495-40d7-ad8c-1f4286f0a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing functions\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a095a5-7413-4210-9aca-a2db6d8dcae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+-------------+------------+\n",
      "|    name|  numbers|first_number|second_number|third_number|\n",
      "+--------+---------+------------+-------------+------------+\n",
      "|    Amit|[1, 2, 3]|           1|            2|           3|\n",
      "|Prashant|[4, 5, 6]|           4|            5|           6|\n",
      "+--------+---------+------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#demonstrating 'getItem'\n",
    "df = df.withColumn(\"first_number\", col(\"numbers\").getItem(0))\n",
    "df = df.withColumn(\"second_number\", col(\"numbers\").getItem(1))\n",
    "df = df.withColumn(\"third_number\", col(\"numbers\").getItem(2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112563e2-5de8-41dc-ba5d-689918ac25d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|account_type|\n",
      "+-----------+------------+\n",
      "|     cust_1|       acc_1|\n",
      "|     cust_2|       acc_2|\n",
      "|     cust_3|       acc_3|\n",
      "|     cust_4|       acc_4|\n",
      "|     cust_1|       acc_1|\n",
      "|     cust_2|       acc_2|\n",
      "|     cust_3|       acc_3|\n",
      "|     cust_4|       acc_4|\n",
      "|     cust_1|       acc_1|\n",
      "|     cust_2|       acc_2|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame creation to demonstrate 'collect_list' and 'collect_set'\n",
    "customized_data = [\n",
    "    (\"cust_1\", \"acc_1\"),\n",
    "    (\"cust_2\", \"acc_2\"),\n",
    "    (\"cust_3\", \"acc_3\"),\n",
    "    (\"cust_4\", \"acc_4\"),\n",
    "    (\"cust_1\", \"acc_1\"),\n",
    "    (\"cust_2\", \"acc_2\"),\n",
    "    (\"cust_3\", \"acc_3\"),\n",
    "    (\"cust_4\", \"acc_4\"),\n",
    "    (\"cust_1\", \"acc_1\"),\n",
    "    (\"cust_2\", \"acc_2\")]\n",
    "columns = [\"customer_id\", \"account_type\"]\n",
    "df = spark.createDataFrame(customized_data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12f4999-cfd4-430f-94e3-2f43d8280366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|customer_id|concat_account_type|\n",
      "+-----------+-------------------+\n",
      "|     cust_1|acc_1, acc_1, acc_1|\n",
      "|     cust_2|acc_2, acc_2, acc_2|\n",
      "|     cust_3|       acc_3, acc_3|\n",
      "|     cust_4|       acc_4, acc_4|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#demonstrating 'collect_list'\n",
    "df_grouped_collect_list = df.groupby(\"customer_id\").agg(sqlfunc.concat_ws(\", \", collect_list(df.account_type)).alias(\"concat_account_type\"))\n",
    "df_grouped_collect_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647079a7-f2f8-419a-804e-a2e4778175d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|customer_id|concat_account_type|\n",
      "+-----------+-------------------+\n",
      "|     cust_1|              acc_1|\n",
      "|     cust_2|              acc_2|\n",
      "|     cust_3|              acc_3|\n",
      "|     cust_4|              acc_4|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#demonstrating 'collect_set'\n",
    "df_grouped_collect_set = df.groupby(\"customer_id\").agg(sqlfunc.concat_ws(\", \", collect_set(df.account_type)).alias(\"concat_account_type\"))\n",
    "df_grouped_collect_set.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
